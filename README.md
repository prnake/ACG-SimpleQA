# ACG-SimpleQA

<!-- <p align="center">
  <img src="image/acg_title.png" width="800px"/>
</p> -->

<p align="center">
   🌐 <a href="https://prnake.github.io/ACG-SimpleQA/" target="_blank">网站</a> • 
   🤗 <a href="https://huggingface.co/datasets/Papersnake/ACG-SimpleQA" target="_blank">Hugging Face</a>
   <br>  
   <a href="README.md">中文</a> | <a href="README_en.md">English</a>
</p>

**ACG-SimpleQA** 是面向中文二次元（ACG，Animation、Comic、Game）领域的客观知识问答数据集，包含 **4242** 条~~自动生成~~精心设计的问答样本。本基准测试旨在评估大语言模型在二次元文化领域的事实性能力，具有中文、多样性、高质量、静态和易评估等特点。

## 📢 最新动态

- **\[2025.04.24\]** 我们正式发布 ACG-SimpleQA 数据集，欢迎访问🤗[Hugging Face](https://huggingface.co/datasets/Papersnake/ACG-SimpleQA) 下载使用

## 💫 简介

ACG-SimpleQA 是一个全面的评估基准，旨在测试大语言模型在二次元文化领域的事实性知识。该数据集具有以下特点：

* 🀄 **中文**：ACG-SimpleQA 专注于中文 ACG 知识，提供了对现有大语言模型在中文二次元领域事实性能力的全面评估。
* 🍀 **多样性**：数据集涵盖动漫、游戏、漫画、音乐等多个子领域，确保评估的全面性。
* ⚡ **高质量**：我们进行了严格的质量控制流程，确保问题的质量和答案的准确性。
* 💡 **静态**：为了保持数据集的长期有效性，所有参考答案不会随时间变化，且知识的截止日期早于 2024 年。
* 🗂️ **易评估**：评估方式与 [SimpleQA](https://github.com/openai/simple-evals) 和 [ChineseSimpleQA](https://github.com/OpenStellarTeam/ChineseSimpleQA) 一致。

## 🔍 研究动机

尽管大语言模型（LLM）在通用知识和推理能力上取得了显著进展，但在二次元（ACG，Animation、Comic、Game）等长尾领域的知识掌握和事实性问答能力仍存在明显短板。我们构建 ACG-SimpleQA 的主要动机如下：

- 现有相关研究的局限：[ChineseSimpleQA](https://github.com/OpenStellarTeam/ChineseSimpleQA) 和 SimpleQA 的动机存在区别，其考察的知识并不长尾，模型区分度不高。目前只有少量关注到 ACG 领域的知识评测，例如 [RoleEval](https://github.com/Magnetic2014/RoleEval) 。

- 现实体验中的模型差异未被量化：在实际使用中，我们观察到如 DeepSeek 等模型在二次元相关领域（如动漫、二游）展现出比 Qwen、GLM 等系列更丰富的知识储备和更强的事实性能力。然而，当前缺乏专门的、公开的榜单来量化和对比主流模型在 ACG 领域的表现。这导致模型训练者难以有针对性地优化和提升模型在该领域的能力。

- 缺乏榜单导致训练语料分布失衡：由于缺乏 ACG 领域的权威评测，模型训练者往往更关注 MMLU、C-Eval、GSM8K、AIME、Codeforces 等教育、数学、代码类 benchmark，导致训练语料中过度采样这些领域的数据，忽视长尾知识。这种短视行为使得模型在主流领域表现优异，但在 ACG 等长尾领域能力严重不足，尤其在国内部分厂商中尤为突出。

- 推动多样性与长尾能力提升：我们希望通过推出 ACG-SimpleQA 等 benchmark，激励模型训练者主动增加 ACG 领域的多样性语料，优化训练 token 分配，从而提升模型在长尾知识、角色扮演等方面的能力。促进模型在更广泛的多样化场景下的应用表现。

<!-- **ACG-SimpleQA 的推出旨在：**

- 弥补现有基准在中文 ACG 领域的空白；
- 量化主流模型在二次元知识上的事实性能力；
- 引导模型训练者关注并提升长尾领域的知识覆盖与表现；
- 推动大语言模型在多样化、长尾场景下的持续进步。 -->

## 🌟 数据来源

* **主要来源**：本数据集的约 99%样本来源于 [萌娘百科](https://zh.moegirl.org.cn)（Moegirlpedia），该平台提供了权威且丰富的二次元作品背景资料。
* **辅助来源**：对于部分小众作品或特殊设定，参考其他权威网站，以保证样本的完整性与准确性。

## 📊 数据分布

以下是数据集中的类别分布：

| 类别 | 样本数量 |
|------|----------|
| 动漫 | 1927     |
| 游戏 | 1817     |
| 漫画 | 240      |
| 音乐 | 63       |
| 其他 | 195      |
| 总计 | 4242     |

## 📋 数据格式

ACG-SimpleQA 数据集采用 JSONL 格式存储，每行一个 JSON 对象，包含以下字段：

```python
{
  "question": "...", // 问题文本
  "answer": "...", // 标准答案
  "category": "...", // 分类（如 动漫、游戏、漫画、音乐、其他 等）
  "urls": ["..."] // 原文链接
}
```

## 🎯 设计原则

ACG-SimpleQA 数据集的构建遵循 SimpleQA 的设计原则，确保每个问题在唯一性、客观性、时效稳定性、挑战性和可验证性方面的高标准。

### 唯一性与明确性

* 每道题目仅对应一个唯一且无争议的答案。
* 问题必须明确限定作品、章节或情节范围，排除二义性，确保答案不含糊。例如，问题中应明确指定作品名称、特定情节或时间点，避免宽泛的表述。

### 客观性

* 问题只涉及可验证的客观事实，答案应源自于原始文本、官方设定或权威资料。
* 禁止涉及主观评价、开放性讨论或任何引导性措辞（如"你认为"或"根据你的观点"）。
* 题目内容应聚焦于可通过查证获得的明确信息，确保答题者依靠知识储备作答，而非依赖推测。

### 时效稳定性

* 答案必须是长期有效的，避免因作品更新或情节变化而影响答案的稳定性。
* 避免使用"截至某年"、"最新一季"等时间性措辞。若需限定时间点，题目应明确指定情节、集数或角色发展状态，以确保答案不会随时间推移而改变。

### 挑战性与知识深度

* 问题应具备一定的挑战性，要求回答者对作品的细节和设定有深入的了解，而非简单复述官方介绍或常见事实。
* 问题应能够考察作品中的细节元素，例如角色背景、事件顺序、设定差异等，而不仅仅是表面的知识。

### 可验证性

* 答案应能在原始文本或权威资源中找到明确的证据。
* 问题设计应确保通过标准的验证渠道（例如萌娘百科等可靠来源）可以确认答案，避免任何无法追溯的猜测。

## 📊 排行榜

<p align="center">
  <img src="static/images/leaderboard.png" width="800px"/>
</p>

| 模型                           | ACG-SimpleQA |
|-------------------------------|--------------|
| gemini-2.5-pro-preview-03-25  | 0.5434       |
| gpt-4.5-preview               | 0.4884       |
| gemini-2.5-flash-preview-04-17| 0.3993       |
| deepseek-v3-241226            | 0.3963       |
| deepseek-v3-250324            | 0.3944       |
| gpt-4.1                       | 0.3880       |
| grok-3-beta                   | 0.3810       |
| chatgpt-4o-latest             | 0.3758       |
| gemini-2.0-flash-001          | 0.3597       |
| minimax-01                    | 0.3175       |
| gemini-2.0-flash-lite-001     | 0.2897       |
| claude-3.7-sonnet             | 0.2864       |
| glm-4-plus                    | 0.2659       |
| claude-3.5-sonnet             | 0.2515       |
| qwen-max                      | 0.2466       |
| doubao-1.5-pro-32k-250115     | 0.2435       |
| grok-3-mini-beta              | 0.2357       |
| o4-mini                       | 0.2263       |
| llama-4-maverick              | 0.1655       |
| gpt-4.1-mini                  | 0.1610       |
| glm-4-air-250414              | 0.1412       |
| claude-3.5-haiku              | 0.1334       |
| gemma-3-27b-it                | 0.1247       |
| llama-3.3-70b-instruct        | 0.1106       |
| qwq-32b                       | 0.0974       |
| qwen2.5-32b-instruct          | 0.0969       |
| gpt-4.1-nano                  | 0.0957       |
| glm-4-flash-250414            | 0.0700       |
| gemma-3-4b-it                 | 0.0370       |

<!-- ## 🛠️ 评估方法 -->

## ❓ 常见问答（QA）

**Q：样本生产流程是怎样的？**  
A：首先从萌娘百科抓取约 15 万条词条，经过正文长度过滤（正文>800），每篇自动生成 5 个 QA 对（共约 50 万条），再对每篇筛选保留 1 个 QA。随后进行难度评分，删除不符合要求的问题/答案（剩约 4 万条），初步用模型（如 gemma-4b、gpt-4o）评测，去除所有模型都能轻松答对的题目，并人工筛选部分样本。之后进行长度筛选（问题<55 字，答案<7 字，全中文答案，答案不重复，约 5000 条），再删除非 ACG 或不符合要求的样本（约 4000 条），最后人工调整，删除容易被判定为违规的内容并人工检查。合成流程后续计划开源。

**Q：为什么有些问题看起来比较奇怪或有问题？**  
A：整体上问题构造风格偏向 SimpleQA，部分问题可能较为冷门或细节化，符合评测目标。极少数异常问题后续会持续标注修复，但不影响整体结论。

**Q：采样范围如何？是否只覆盖热门作品？**  
A：采样整体较为平衡，热门番剧、二游词条数量较多，因此相关问题也更多，同时也保留了大量长尾作品和设定，确保多样性。

**Q：是不是模型把萌娘百科背下来就行？**  
A：并非如此。SimpleQA 中 80%的数据来自 Wikipedia，所有主流模型都训练过，但效果依然有限。模型需要足够大，并且多次见过不同版本的相关概念，才能在此类任务中表现优异。预训练阶段需要对相关主题进行大量采样和上采样，单纯“背诵”并不能解决问题。

**Q：评测成绩和模型规模、训练策略有什么关系？**  
A：我们观察到，SimpleQA 类榜单的成绩与模型参数规模有一定正相关关系，但合理的训练 token 分配和语料多样性同样能带来超越平均水平的表现。例如，相对较小的 `gemini-2.5-flash-preview` 在 ACG-SimpleQA 榜单上取得了优异成绩。这进一步说明，专门的 ACG 评测基准有利于引导优化模型预训练数据的选择策略、提升长尾能力。

## 📜 引用

如果你的研究过程中使用了该仓库，请考虑引用：

```bibtex
@misc{pka2025acgsimpleqa,
    title={ACG-SimpleQA},
    author={Papersnake},
    howpublished = {\url{https://github.com/prnake/ACG-SimpleQA}},
    year={2025}
}
```

## 🤝 贡献

我们欢迎社区贡献！如果您发现任何问题或有改进建议，请提交 issue 或 pull request。

## 📄 许可证

本项目遵循 MIT 许可证。详情请参阅 LICENSE 文件。